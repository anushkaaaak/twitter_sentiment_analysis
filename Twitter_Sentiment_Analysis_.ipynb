{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Sentiment Analysis\n",
        "\n",
        "**Objective:** Classify tweets as Positive, Negative, or Neutral using NLP and Transformer models.  \n",
        "**Dataset:** 1,03,250+ training tweets, 16k+ validation tweets.  \n",
        "**Model:** DistilBERT fine-tuned for sequence classification.  \n",
        "\n",
        "**Workflow Highlights:**\n",
        "- Data preprocessing & label mapping\n",
        "- Tokenization with HuggingFace tokenizer\n",
        "- Model fine-tuning with Trainer API\n",
        "- Evaluation & metrics visualization\n",
        "- WordCloud analysis per sentiment"
      ],
      "metadata": {
        "id": "wZ5LARl3fbS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1 : Import Libraries & Setup"
      ],
      "metadata": {
        "id": "JwVBDnGMfh28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw6x8QqlfHzy"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# HuggingFace Transformers & Datasets\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2 : Upload & Preprocess Data"
      ],
      "metadata": {
        "id": "0WPkIThffk7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload CSVs\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Define column names\n",
        "cols = ['id', 'entity', 'sentiment', 'text']\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('twitter_training.csv', names=cols, header=0)\n",
        "val_df   = pd.read_csv('twitter_validation.csv', names=cols, header=0)\n",
        "\n",
        "# Map labels to integers & drop 'Irrelevant'\n",
        "label_map = {'Negative':0, 'Positive':1, 'Neutral':2}\n",
        "train_df = train_df[train_df['sentiment'].isin(label_map.keys())]\n",
        "val_df   = val_df[val_df['sentiment'].isin(label_map.keys())]\n",
        "train_df['label'] = train_df['sentiment'].map(label_map)\n",
        "val_df['label']   = val_df['sentiment'].map(label_map)\n",
        "\n",
        "# Ensure text is string & handle NaN\n",
        "train_df['text'] = train_df['text'].astype(str).fillna('')\n",
        "val_df['text']   = val_df['text'].astype(str).fillna('')\n",
        "\n",
        "# Quick check\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "R1fXi_HWfvMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 3 : Convert to HuggingFace Dataset & Tokenize"
      ],
      "metadata": {
        "id": "x7p-n1Fuf2mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Dataset\n",
        "train_ds = Dataset.from_pandas(train_df[['text','label']])\n",
        "val_ds   = Dataset.from_pandas(val_df[['text','label']])\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# Apply tokenization\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "val_ds   = val_ds.map(tokenize, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_ds.set_format('torch', columns=['input_ids','attention_mask','label'])\n",
        "val_ds.set_format('torch', columns=['input_ids','attention_mask','label'])\n"
      ],
      "metadata": {
        "id": "xW54SqNXf9l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 4 : Load Model"
      ],
      "metadata": {
        "id": "npIlraThgBNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(label_map)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "0djx_o6BgI_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 5 : Training Arguments & Metrics"
      ],
      "metadata": {
        "id": "P0hKPdMYgRBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics function\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_metrics_simple(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "0tSf9PlggXfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 6 : Train the Model"
      ],
      "metadata": {
        "id": "CGRFnvK6gdFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics_simple\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "fqwKbUROgirm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 7: Evaluate Model"
      ],
      "metadata": {
        "id": "igLjHl_igpX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(\"\\n Evaluation Results:\")\n",
        "for k, v in eval_results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "K2sh4HBLglee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 8 : Save Model & Tokenizer"
      ],
      "metadata": {
        "id": "KD__BddCgw1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./sentiment_model\")\n",
        "print(\"\\n Model and tokenizer saved to ./sentiment_model\")\n"
      ],
      "metadata": {
        "id": "jvqGsjsXg1wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 9 : Sample Predictions"
      ],
      "metadata": {
        "id": "4Rsdm3_Dg5vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {v:k for k,v in label_map.items()}\n",
        "\n",
        "def predict_sentiment(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model(**inputs)\n",
        "    preds = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "    return [id2label[p] for p in preds]\n",
        "\n",
        "sample_texts = [\n",
        "    \"I absolutely love the new update, great job!\",\n",
        "    \"Worst experience ever, I am so disappointed.\",\n",
        "    \"It is okay, neither good nor bad.\",\n",
        "    \"The bug in the app keeps crashing, very frustrating.\",\n",
        "    \"Thank you for the amazing support, really appreciated!\"\n",
        "]\n",
        "\n",
        "predictions = predict_sentiment(sample_texts)\n",
        "for text, pred in zip(sample_texts, predictions):\n",
        "    print(f\"{text}  -->  {pred}\")\n"
      ],
      "metadata": {
        "id": "1rYi-Qz7g_ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 10 : WordCloud Visualization"
      ],
      "metadata": {
        "id": "BuGJ0zbthDF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_wordcloud(df, sentiment_label, max_words=50):\n",
        "    text = \" \".join(df[df['label'] == sentiment_label]['text'].astype(str))\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\", max_words=max_words).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud for {id2label[sentiment_label]} Tweets\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "    words = text.split()\n",
        "    freq_dist = pd.Series(words).value_counts().head(15)\n",
        "    print(f\"Top words for {id2label[sentiment_label]}:\")\n",
        "    print(freq_dist)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Plot for each sentiment\n",
        "for label in range(len(label_map)):\n",
        "    plot_wordcloud(train_df, label)\n"
      ],
      "metadata": {
        "id": "ywY0u4yLhJEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}